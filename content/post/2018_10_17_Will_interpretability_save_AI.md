---
title: "Will Interpretability save AI?"
author: "Alicja Gosiewska and Mateusz Staniak"
date: 2018-10-17
categories: [""]
tags: ["xai", "iml", "meta", "hello world"]
---


Even though work on Artificial Intelligence began in 1956 and the field didn’t really take off.

Well, it did, but not the way the founding fathers anticipated:
the great success of AI was thanks to statistics, not formal logic.

<!--However,  its progress was still constrained by a lack of computing power.  Rapid advance of computer technology removed those inhibitions.
-->
During its development, AI has already gone through two periods known as AI winters (1974 - 1980 and 1987 -1993).
They were the result of not meeting the exaggerated expectations in fields like automatic translation and general AI (AGI).

Spring came when new applications or approaches to AI were discovered.
AI has went from winning chess games and quiz shows to [beating the GO world champion](https://www.theverge.com/2017/10/18/16495548/deepmind-ai-go-alphago-zero-self-taught), [composing original music](http://fredrikstenbeck.com/watson-beat/) and [indentifying new planets](https://www.wired.com/story/new-kepler-exoplanet-90i-discovery-fueled-by-ai/).


The next winter is coming due to a lack of interpretability.
The black box nature of statistical methods such as neural networks results in barriers in AI development in regulated areas such as banking and health services. There were several failed attepts to use black box models in medicine, e.g. [IBM’s Watson supercomputer which recommended ‘unsafe and incorrect’ cancer treatments](https://www.statnews.com/2018/07/25/ibm-watson-recommended-unsafe-incorrect-treatments/) that 
and [Google Flu that predicted trends in flu cases](https://www.theguardian.com/technology/2014/mar/27/google-flu-trends-predicting-flu).
It also caused reaffirmation of human biases based on gender or race. 
Faulty data collection strongly impacts models, as in the case of [Amazon recruiting tool that showed bias against women](https://www.telegraph.co.uk/technology/2018/10/10/amazon-scraps-sexist-ai-recruiting-tool-showed-bias-against/).
For more insights see [the interview with Cathy O'Neil](https://qz.com/819245/data-scientist-cathy-oneil-on-the-cold-destructiveness-of-big-data/). 

These issues often result in lack of trust in automatic decisions on the business and end-user side.
Explainers of complex Machine Learning models are the hope for a new era of AI development.
This blog will serve as a weekly review of advances in Interpretable Machine Learning.
We will show methods that help validate and interpret model predictions.

Stay tuned!